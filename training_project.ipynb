
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n\n# Пути\nMODEL_NAME = \"guidobenb/DarkBERT-finetuned-ner\"  # Выбранная модель\nDATASET_PATH = \"./datasets\"  # Папка с локальными датасетами\nOUTPUT_DIR = \"./trained_model\"  # Папка для сохранения модели\nLOG_FILE = \"./training.log\"  # Файл логов\n\n# Параметры обучения\nNUM_EPOCHS = 3\nBATCH_SIZE = 8\nLEARNING_RATE = 5e-5\n\n# Параметры железа\nUSE_GPU = torch.cuda.is_available()\nDEVICE = \"cuda\" if USE_GPU else \"cpu\"\nNUM_WORKERS = 4  # Количество потоков загрузки данных\nRAM_LIMIT = \"32GB\"  # Ограничение ОЗУ (необязательно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport json\nfrom datasets import load_dataset\n\n\ndef load_data(source=\"huggingface\", dataset_name=\"kaykyramos/onlyfans-preferences\"):\n    \"\"\"\n    Загружает данные для обучения.\n    :param source: \"huggingface\" или \"local\"\n    :param dataset_name: название датасета (для Hugging Face) или путь к локальному файлу\n    \"\"\"\n    if source == \"huggingface\":\n        print(f\"Загружаем датасет {dataset_name} с Hugging Face...\")\n        dataset = load_dataset(dataset_name)\n    else:\n        print(f\"Загружаем локальный датасет из {dataset_name}...\")\n        if not os.path.exists(dataset_name):\n            raise FileNotFoundError(f\"Файл {dataset_name} не найден!\")\n        with open(dataset_name, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        dataset = {\"train\": data}  # Преобразуем в формат, похожий на Hugging Face\n\n    print(\"Данные загружены!\")\n    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логирование и проверка повторного обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n\n\ndef get_data_hash(dataset):\n    \"\"\"\n    Создает хеш от данных, чтобы проверить, не обучалась ли модель на них ранее.\n    \"\"\"\n    data_str = json.dumps(dataset, sort_keys=True)\n    return hashlib.md5(data_str.encode()).hexdigest()\n\n\ndef check_if_trained(dataset_hash):\n    \"\"\"\n    Проверяет, обучалась ли модель на этих данных.\n    \"\"\"\n    if os.path.exists(LOG_FILE):\n        with open(LOG_FILE, \"r\") as f:\n            trained_hashes = f.read().splitlines()\n        return dataset_hash in trained_hashes\n    return False\n\n\ndef log_training(dataset_hash):\n    \"\"\"\n    Записывает хеш датасета в лог, чтобы избежать повторного обучения.\n    \"\"\"\n    with open(LOG_FILE, \"a\") as f:\n        f.write(dataset_hash + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, TrainingArguments, Trainer\n\n\ndef train_model(dataset):\n    \"\"\"\n    Обучает модель на указанном датасете.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME).to(DEVICE)\n\n    # Токенизация данных\n    tokenized_datasets = dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True)\n\n    # Проверяем, обучалась ли модель на этом датасете\n    dataset_hash = get_data_hash(tokenized_datasets[\"train\"])\n    if check_if_trained(dataset_hash):\n        print(\"⚠️ Эта модель уже обучалась на данных. Пропускаем обучение.\")\n        return\n\n    training_args = TrainingArguments(\n        output_dir=OUTPUT_DIR,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        num_train_epochs=NUM_EPOCHS,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        learning_rate=LEARNING_RATE,\n        logging_dir=\"./logs\",\n        logging_steps=500,\n        save_total_limit=2,\n        fp16=USE_GPU,\n        dataloader_num_workers=NUM_WORKERS\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets.get(\"test\", None),\n        tokenizer=tokenizer\n    )\n\n    trainer.train()\n    trainer.save_model(OUTPUT_DIR)\n    log_training(dataset_hash)\n    print(f\"✅ Модель сохранена в {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Интерактивная CLI-панель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Интерактивная настройка обучения DarkBERT\")\n    parser.add_argument(\"--source\", choices=[\"huggingface\", \"local\"], default=\"huggingface\", help=\"Источник данных\")\n    parser.add_argument(\"--dataset\", type=str, default=\"kaykyramos/onlyfans-preferences\", help=\"Название датасета или путь\")\n    parser.add_argument(\"--epochs\", type=int, default=3, help=\"Количество эпох\")\n    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Размер батча\")\n    parser.add_argument(\"--lr\", type=float, default=5e-5, help=\"Скорость обучения\")\n\n    args = parser.parse_args()\n\n    # Загружаем данные\n    dataset = load_data(args.source, args.dataset)\n\n    # Обучаем модель\n    train_model(dataset)\n\nif __name__ == \"__main__\":\n    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
